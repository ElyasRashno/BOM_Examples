title,type,spdxId,creationInfo,name,builtTime,originatedBy,software_downloadLocation,dataset_confidentialityLevel,dataset_dataCollectionProcess,dataset_dataPreprocessing,comment,dataset_datasetAvailability,dataset_datasetSize,dataset_datasetType,dataset_intendedUse,dataset_anonymizationMethodUsed,dataset_datasetnoise,dataset_hasSensitivePersonalInformation,dataset_UpdateMechanism,dataset_knownBias,dataset_description,dataset_sensor,dataset_homePage,dataset_license,Sources,Sources1
pmc-llama-medicationqa_BOM,,Unknown,2023-11-23T08:47:30+00:00,axiong/pmc_llama_instructions,2023-09-01T00:56:32+00:00,axiong,https://huggingface.co/axiong/pmc_llama_instructions,Unknown,Not publicly available,No information provided,Unknown,public,5366618.90 KB,Expected to be structured for medication question answering tasks,"['question-answering', 'text-generation']",Unknown,Cannot be determined without additional information,Unclear if present in the dataset,Information not disclosed,Not publicly documented,"The PMC-Llama-MedicationQA dataset is likely focused on medication-related questions and answers, designed to assist with medical queries. Further details about its content and structure are currently unavailable.",No sensor data associated with this dataset.,https://huggingface.co/axiong/pmc_llama_instructions,openrail,commitpackft-hlsl_BOM,commitpackft-postscript_BOM
commitpackft-lua_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,public,"The dataset is collected from public repositories, such as GitHub, GitLab, and Bitbucket. The collection process involves extracting commit history and associated code changes for Lua projects.","The commits are processed to extract relevant information such as the commit message, changed files, and diffs. Personal identifiable information (PII) or sensitive data is anonymized during preprocessing if present.",Unknown,public,34280013.98 KB,"Software engineering, version control, code analysis",Unknown,"Anonymization techniques include removal of personal identifiers from commit messages, user names, and email addresses. Sensitive information such as API keys or credentials is also redacted during preprocessing.","The dataset may contain some noise due to the inclusion of various Lua projects with different coding practices, documentation styles, and code quality. Efforts are made to clean and normalize the data during preprocessing to reduce noise.",No sensitive personal information is included in the dataset as all identifiable information has been anonymized or removed during preprocessing.,"As of now, there is no update mechanism for this dataset. It is a one-time release based on the data collected up to a certain point in time.","The dataset may reflect inherent biases present in open-source projects, such as language-specific coding practices, project popularity, and developer community dynamics. Efforts are made to mitigate these biases during collection and preprocessing.","commitpackft-lua is a dataset of annotated commits focused on Lua programming language, derived from the(commitpackft). It captures code changes and associated information to support research in software development, bug fixing, and code improvement techniques.",No sensors are involved in this dataset; it is purely composed of code changes and commit information.,https://arxiv.org/abs/2308.07124,mit,,
metamathqa-MATH_AnsAug_BOM,,Unknown,2023-12-21T01:35:53+00:00,meta-math/MetaMathQA,2023-09-21T17:22:46+00:00,meta-math,https://huggingface.co/meta-math/MetaMathQA,Public,"The MetaMathQA dataset was created through a process of selecting questions from existing mathematical reasoning datasets such as GSM8K and MATH. These questions were then augmented using various techniques including answer augmentation (AnsAug), rephrasing, First Order Augmentation by Rephrasing (FOBAR), and Supervised Verbosity (SV). The augmentation processes aimed to improve the dataset's diversity and effectiveness for fine-tuning language models on mathematical reasoning tasks.","Data preprocessing involved applying multiple augmentation techniques such as Answer Augmentation (AnsAug) to generate diverse answer variations, rephrasing questions to increase question variability, FOBAR to create first-order logical forms from natural language questions, and SV to adjust the verbosity of the questions. The preprocessing also included merging augmented data with original datasets to enhance model robustness.",Unknown,public,9855173.58 KB,Text-based dataset containing mathematical problems and their solutions.,Unknown,No specific anonymization methods mentioned in the document.,"While the dataset includes various augmentations, there is no explicit mention of noise reduction or denoising techniques applied during preprocessing.","None identified. The dataset focuses on mathematical problems and their solutions, not containing any personal or sensitive information.",No specific update mechanism mentioned in the document.,May include inherent biases from the source datasets (GSM8K and MATH). No explicit mitigation methods for bias are described.,"MetaMathQA is an augmented dataset created to improve mathematical reasoning tasks by enhancing question variety and answer diversity through techniques such as Answer Augmentation, rephrasing, FOBAR, and SV. It builds upon existing datasets like GSM8K and MATH, aiming to provide a robust training resource for model fine-tuning.",Unknown,https://meta-math.github.io,mit,,
tsi-gen_debiased_nli-mnli_par_z_BOM,,Unknown,2022-04-25T09:49:52+00:00,pietrolesci/gen_debiased_nli,2022-04-25T09:35:37+00:00,pietrolesci,https://huggingface.co/pietrolesci/gen_debiased_nli,Public,"Synthetic text generation using NLI algorithms with a focus on debiasing to minimize language bias, particularly by removing gendered references and other demographic markers.",Preprocessing included language neutralization techniques and the removal of biased elements such as gender-specific terms. The dataset was generated de novo to address biases inherent in existing datasets.,Unknown,public,6718214.92 KB,Text corpus,Unknown,Anonymization techniques focused on neutralizing language by removing gendered references and other demographic markers to prevent bias in natural language processing tasks.,Minimal noise as synthetic generation ensures controlled data quality. All examples appear well-formed with no evident errors or noise.,None; the dataset avoids inclusion of any personally identifiable information (PII) through anonymization techniques and synthesis from scratch without real-world sourcing.,"No specific update mechanisms are mentioned, but further inquiries would need to be directed to the dataset providers for details on updates or versions.","Known biases addressed include language bias, particularly gendered references. The dataset was crafted to mitigate these issues in existing natural language datasets used for NLI tasks.","A synthetic dataset designed for natural language inference (NLI) tasks based on the MNLI format. Created with an emphasis on reducing bias by avoiding gender-specific terms and other demographic markers, ensuring neutrality in its text generation.",N/A - Purely synthetic dataset without physical sensor data involvement.,https://huggingface.co/pietrolesci/gen_debiased_nli,Unknown,commitpackft-fortran_BOM,
tsy-imppres-presupposition_question_presupposition-presupposition_BOM,,Unknown,2023-06-21T12:52:43+00:00,tasksource/imppres,2023-01-05T20:14:45+00:00,tasksource,https://huggingface.co/tasksource/imppres,Unknown,Unknown,Unknown,Unknown,public,108226.99 KB,Unknown,['text-classification'],Unknown,Unknown,Unknown,Unknown,Unknown,"This repository houses the IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of >25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types. IMPPRES is an NLI dataset following the format of SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018) and XNLI (Conneau et al., 2018), which was created to determine how well trained NLI models do on recognizing several kinds of presuppositions and scalar implicatures.",Unknown,https://github.com/facebookresearch/Imppress,apache-2.0,,
commitpackft-zephir_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,"Data is collected from publicly available repositories, focusing on identifying and extracting meaningful commits that address bugs or improve code quality.","The data undergoes several preprocessing steps, including filtering out trivial changes, normalizing code and diff structures, cleaning the commit messages for clarity, and ensuring consistency in code representation across different programming languages.",Unknown,public,34280013.98 KB,Text-based dataset with structured code changes (diffs) and associated metadata such as commit messages.,Unknown,"Usernames, email addresses, and project names are anonymized. Specific sensitive information is removed or pseudonymized if it appears within the commit messages or code.","Noise is minimized by filtering out non-functional changes such as formatting adjustments, comment edits, and other trivial modifications. A postprocessing step ensures that only meaningful changes are included for analysis.","Minimal sensitive personal information exists, primarily in commit messages. All sensitive details are removed or anonymized during preprocessing.",The dataset is periodically updated to include new commits from active repositories. Updates are guided by feedback from the research community and improvements in data filtering techniques.,"Biases may exist towards languages with large open-source communities, common issues across projects, and code changes that are easily identifiable as bug fixes or quality improvements. There is also potential bias based on the selection of repositories used for data collection.","This dataset contains a curated collection of commits from various programming language projects, focusing on functional changes aimed at fixing bugs or enhancing code quality. It is designed to aid in training models for automated code improvement tasks。",N/A,https://arxiv.org/abs/2308.07124,mit,,
tsy-robust_nli-pi_cd_BOM,,Unknown,2022-04-25T11:45:07+00:00,pietrolesci/robust_nli,2022-04-25T11:43:30+00:00,pietrolesci,https://huggingface.co/pietrolesci/robust_nli,Unknown,Unknown,Unknown,Unknown,public,169122.39 KB,text classification for natural language inference,Unknown,Unknown,Unknown,Unknown,Unknown,"The dataset is created to address known biases in natural language inference tasks and focuses on permutation invariance. The main known biase include social bias, hidden data bias, selection bias.",The dataset is focused on robust NLI methods with an emphasis on permutation invariance and debiasing techniques,Unknown,https://huggingface.co/docs/datasets,Apache License 2.0,,
tsi-parade_BOM,,Unknown,2023-05-31T08:20:40+00:00,tasksource/parade,2023-05-30T07:42:30+00:00,tasksource,https://huggingface.co/tasksource/parade,Unknown,Unknown,Unknown,Unknown,public,22690.72 KB,Unknown,"['sentence-similarity', 'text-classification']",Unknown,Unknown,Unknown,Unknown,Unknown,"code and dataset of EMNLP 2020 paper ""PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge""",Unknown,https://huggingface.co/tasksource/parade,Unknown,tsi-lingnli_BOM,
commitpackft-fortran_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,"The dataset was collected by extracting code commits from various Fortran projects hosted on public repositories such as GitHub, GitLab, and Bitbucket. The extraction process involved automated scripts to retrieve commit metadata, including changes made, timestamps, and contributor information.","Preprocessing steps included parsing commit messages, cleaning the data to remove irrelevant information, tokenizing code snippets for consistent representation, and normalizing contributors' details to maintain uniformity across entries. Any malformed data or duplicate commits were also filtered out during this phase.",Unknown,public,34280013.98 KB,"Text, Code",Unknown,"Contributors' personal details were anonymized by replacing usernames with generic identifiers. Email addresses were masked, and any direct references to individuals or organizations were removed to protect privacy.","Noise in the dataset includes minor syntax errors introduced during commit extraction and changes unrelated to core functionality, such as whitespace adjustments. These have been minimized through rigorous data cleaning procedures.",No sensitive personal information is included in the dataset. Anonymization techniques ensure that contributor details are not directly identifiable.,"The dataset is updated periodically, with new commits added every six months to reflect recent developments in Fortran projects. Contributors are encouraged to report any issues or suggest improvements through public issue trackers.","Potential biases include a focus on popular open-source projects and an underrepresentation of less common Fortran dialects. The dataset may also be skewed towards projects active during specific time periods, potentially overlooking historical Fortran implementations.",commitpackft-fortran is a collection of code commits from Fortran projects intended for studying software evolution and improving automated code generation tools. It serves as a resource for researchers working on program synthesis and analysis techniques.,N/A,https://arxiv.org/abs/2308.07124,mit,,
commitpackft-pov-ray-sdl_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,public,Collected from public repositories using GitMining tool.,"Filtered commits to get minimal diffs focusing on code changes relevant to function fixes, explanations, and generation in multiple languages. Multiple file changes are reduced by considering only the main changed file per commit, ensuring balance across programming languages.",Unknown,public,34280013.98 KB,Code-focused dataset involving source code and commit diffs for training models on specific programming tasks.,Unknown,"Removed URLs, personal details, and commit hashes to protect privacy.",Minimized by focusing on relevant code changes with clear before/after diffs.,Excluded after preprocessing.,Periodically updated as new open-source projects emerge.,Potential bias towards popular programming languages and project types present on GitHub.,"A collection of commits for function fixes, explanations, and code generation tasks in various programming languages. Includes diffs showing human-evaluated improvements by maintainers or contributors.",N/A,https://arxiv.org/abs/2308.07124,mit,,
commitpackft-tex_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,public,"Data was collected through Git repositories, specifically GitHub, using tools like GHTorrent or custom scripts to extract commit information and modifications.","The data underwent a preprocessing phase where it was cleaned and aligned with specific code modification tasks. This included filtering out irrelevant files, ensuring relevance to the intended tasks (e.g., bug fixing), and organizing commits into a structured format suitable for instruction-tuning Code LLMs.",Unknown,public,34280013.98 KB,Unknown,Unknown,"GitHub's inherent anonymization of usernames, followed by additional anonymization steps applied during the dataset preparation to further remove any identifiable information.","Low noise after preprocessing. The corpus focuses on meaningful code changes relevant to specific programming tasks, ensuring minimal extraneous data.","Minimal sensitive information present, as GitHub handles user details with anonymization measures in place。",Periodically updated with new commits fromGitHub repositories. OCTOGEEX models are re-trained with fresh data sourced similarly to maintain relevance and accuracy.,Potential language bias toward English and dominant programming languages due to the nature of GitHub's usage patterns. Limited support for less common programming languages may exist due to lower contribution volumes.,"A corpus comprising code changes fromcommits on GitHub, used for instruction-tuning large language models (Code LLMs) to enhance their performance across a variety of programming tasks, including bug fixing and documentation editing.",Unknown,https://arxiv.org/abs/2308.07124,mit,,
tsy-autotnli_BOM,,,2023-05-31T08:55:41+00:00,tasksource/autotnli,2023-02-07T21:36:51+00:00,tasksource,https://huggingface.co/tasksource/autotnli,public,"The dataset was created by systematically varying parameters across different categories (e.g., Person, City) using predefined templates and synthetic data generation techniques to produce entailment relations.","The dataset was preprocessed to create training instances for textual entailment tasks, involving the systematic variation of attributes across different categories.",Unknown,public,821574.91 KB,"Textual entailment dataset for NLP tasks, specifically designed to evaluate AI models on recognizing entailment relations.",['text-classification'],Not discussed in the provided context; likely not applicable since it uses synthetic data with no personal information.,"Synthetic dataset, so it is designed to have controlled and minimal noise. May include variations based on reasoning types but no inherent noise.","No sensitive personal information included; focuses on concepts like Person, City, University, etc., without personal data.",Not specified in the context provided; possibly not available as updates are typically for dynamic datasets which this is not.,"Potential bias from the distribution of reasoning types (e.g., more simple lookup and lexical examples), but balanced across different entailment labels.","A synthetic textual entailment dataset generated using templates, designed to study AI models' ability to handle various reasoning tasks in natural language processing.",No physical sensors used; entirely text-based.,https://autotnli.github.io/,apache-2.0,,
fc-sni-gap_BOM,,Unknown,2018-10-18T17:48:21+00:00,gap_coreference,2020-11-10T12:00:12+00:00,google-research-datasets,https://github.com/google-research-datasets/gap-coreference.git,Unknown,Unknown,Unknown,Unknown,public,1102 KB,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,http://goo.gl/language/gap-coreference,Unknown,https://www.kaggle.com/datasets/owen1226/gap-coreference,Apache License 2.0,,
commitpackft-hlsl_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,The dataset was collected through automated tools that gathered code snippets from public repositories and issue tracking systems. This included pulling examples of code before and after fixes were applied.,"The raw data underwent cleaning processes to remove redundant or incomplete commits. Normalization of code syntax and context, along with extraction of commit descriptions helped in creating structured datasets.",Unknown,public,34280013.98 KB,Text-based dataset consisting of before-and-after code snippets with associated commit descriptions.,Unknown,"Anonymized by removing user identities and sensitive project details, focusing solely on code content with context.","Known to have issues like duplicate entries, incorrect or incomplete fixes, and ambiguous commit messages that reduce its utility for certain applications.",Does not contain personal data beyond contributor anonymous identifiers if any. Primarily contains code-related information.,No active updates as of its last release due to the challenges faced by the project team in maintaining accuracy and relevance.,"Biased towards projects with more frequent commits, certain programming languages, and particular types of issues commonly addressed in open-source projects.",A collection of code snippets paired with commit messages aiming to demonstrate fixes for identified bugs. Primarily used for training or evaluating models on code correction tasks.,Unknown,https://arxiv.org/abs/2308.07124,mit,commitpackft-solidity_BOM,
commitpackft-solidity_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,"The dataset was collected by analyzing GitHub repositories to extract code changes (commits) specifically in Solidity files, with a focus on functional and bug-fixer commits.","Preprocessing involved automated cleaning, normalization, and formatting of the extracted Solidity code. Changes were systematically filtered based on their relevance to specific programming tasks related to smart contracts.",Unknown,public,34280013.98 KB,Text,Unknown,None,Low,None,"Periodically updated to include new commits from open-source repositories, but without a real-time update feature.","Potential bias may exist towards projects with more active communities and contributions. Additionally, the dataset might overrepresent certain coding patterns or common mistakes frequently addressed in Solidity smart contracts. There could also be a language bias due to the dominance of English in commit messages and code documentation.","This dataset comprises functional and bug-fixing commits in Solidity programming language from various open-source projects. It is specifically curated for training AI models to understand and generate code fixes, focusing on enhancing their capability to handle smart contracts effectively.",None,https://arxiv.org/abs/2308.07124,mit,commitpackft-html+erb_BOM,book-summaries_BOM
tsi-snips_built_in_intents_BOM,,Unknown,2016-12-23T11:03:58+00:00,ATIS Airline Travel Information System,2020-09-23T15:55:14+00:00,sonos,https://github.com/sonos/nlu-benchmark.git,Unknown,Unknown,Unknown,Unknown,public,1222 KB,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,ATIS Intent Classification Dataset,Unknown,https://www.kaggle.com/datasets/hassanamin/atis-airlinetravelinformationsystem,Creative Commons Zero v1.0 Universal,,
commitpackft-groovy_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,"Extracted from public GitHub and GitLab repositories, focusing on commits related to the Groovy programming language.","Anonymized usernames, repository names, and emails. Cleaned and validated commits to ensure quality and consistency.",Unknown,public,34280013.98 KB,"Code change dataset with structured information about commits, diffs, original code, and modified code.",Unknown,"Anonymization of usernames, repository names, emails, and IP addresses.",Minimal noise due to filtering invalid or incomplete commits.,None; all sensitive data has been anonymized.,Periodically updated with new commits and improvements.,"Potential bias towards popular or active open-source projects in the Groovy ecosystem, which may not fully represent all use cases.","A collection of code changes (commits) in Groovy across various open-source projects, focusing on modifications to functions, bug fixes, and feature enhancements. The dataset emphasizes community contributions and specific features related to the Groovy language.",N/A,https://arxiv.org/abs/2308.07124,mit,,
oasst-en_BOM,,Unknown,2023-05-02T13:21:21+00:00,OpenAssistant/oasst1,2023-04-13T15:48:16+00:00,OpenAssistant,https://huggingface.co/OpenAssistant/oasst1,Public,Data was collected via a conversation tree approach where volunteers contributed prompts and replies using an open-source web interface. Conversations were built in multiple languages and labeled with scores for quality control.,"Conversations underwent anonymization to remove personal information, utilizing NLP techniques. Replies were subject to initial filtering to ensure safety and quality before integration into the dataset.",Unknown,public,1540076.51 KB,The dataset is text-based and focuses on conversational data formatted as dialogue between multiple participants.,Unknown,Anonymization involved removing personal information through natural language processing techniques during data preparation。,"Moderate noise exists due to volunteer contributions, though replies are checked for safety and quality post-collection.",Checked for toxic or sensitive content; such entries are removed. Minimal risk of remaining traces given imperfect detection methods.,"Initial collection was a one-time effort in 2022-2023, but the dataset can be expanded through community contributions.",Possible language and cultural biases towards English-speaking contributors. Topic representation may reflect the interests and availability of volunteers rather than global diversity.,"A large-scale conversational dataset in multiple languages built using a tree methodology with volunteer interactions, scored replies for high-quality conversation trees useful for training AI models.",None,https://open-assistant.io,apache-2.0,,
tsi-lingnli_BOM,,Unknown,2023-05-31T08:40:53+00:00,tasksource/lingnli,2023-01-11T20:59:56+00:00,tasksource,https://huggingface.co/tasksource/lingnli,Public,"Constructed through iterated rounds of collection and curation by linguists and crowdworkers following principles in linguistic typology, stylistics, lexicography, and composition. Data is collected for each premise across three different annotation protocols (Baseline, LitL, LitL Chat), ensuring diversity in hypothesis creation according to specific creativity heuristics.","Each generated hypothesis is validated by expert linguists for clarity, correctness, and alignment with the desired linguistic principles.",Unknown,public,103879.20 KB,"Textual dataset focusing on linguistics and natural language analysis. It includes premises and corresponding hypotheses across three different annotation protocols (Baseline, LitL, LitL Chat).",['text-classification'],No explicit anonymization methods are described since the dataset examples do not contain personal information or identifiable data points.,Minimal noise is expected due to the careful curation and validation process involving expert linguists.,None; the dataset primarily contains abstract linguistic constructions without personal data.,"No explicit update mechanism is discussed, but future updates could follow similar collaborative processes with linguist oversight if additional data collection rounds are conducted.","Potential biases may stem from the inherent focus on specific linguistic principles and creativity heuristics, which might favor certain types of language over others. The dataset's construction involves human subjective judgment in hypothesis creation, introducing potential personal bias despite efforts to minimize it.","The dataset is constructed for linguistic analysis within natural language processing contexts. It focuses on diverse linguistic strategies across multiple annotation protocols, emphasizing creativity and adherence to specific linguistic principles.",None; the dataset is purely textual in nature.,https://huggingface.co/tasksource/lingnli,unknown,,
tsi-nan_nli-joey234__nan_nli_BOM,,Unknown,2022-10-13T23:18:18+00:00,joey234/nan-nli,2022-10-13T23:16:18+00:00,joey234,https://huggingface.co/joey234/nan-nli,Public,"The dataset was collected through manual annotation and expert linguistic analysis, focusing on creating samples that manipulate negation scopes and contexts in natural language understanding tasks.","Data preprocessing involved crafting premises and hypotheses with careful consideration of negation, leveraging existing datasets like MNLI (Multi-Genre Natural Language Inference) and NegNLI.",Unknown,public,7278.72 KB,Textual,['text-classification'],None (the dataset consists of generic text samples without personal or sensitive information),Low (samples are carefully constructed to minimize noise and focus on negation challenges),None,"Not specified in the documentation, but updates may be added periodically as new findings emerge.","The dataset may have language-specific biases, as it was designed for use with English-based natural language understanding tasks. Additionally, expert annotations could introduce subjective biases.",TSI-NAN-NLI is a research dataset focused on analyzing and improving models' handling of negation in natural language understanding tasks. It includes samples where negation plays a critical role in determining the logical relationships between text segments。,None (the dataset does not involve sensor data),https://huggingface.co/joey234/nan-nli,['cc-by-sa-4.0'],,
tsy-babi_nli-simple_negation_BOM,,Unknown,2024-06-19T12:03:10+00:00,tasksource/babi_nli,2023-01-01T14:39:33+00:00,tasksource,https://huggingface.co/tasksource/babi_nli,Public,Synthetic stories created using templates without real user data.,Text normalization and standardization for NLP tasks.,Unknown,public,403547.52 KB,N/A,['text-classification'],Not applicable (synthetic dataset).,Low noise due to controlled synthetic generation.,"None present, as the dataset is synthetic.",Static; no real-time updates or incremental additions.,May exhibit limitations in generalizability to diverse real-world scenarios.,"A synthetic NLP benchmark dataset used for developing and testing Memory Networks (MemNNs) on tasks involving simple negation, based on the bAbI framework. Publicly available due to absence of personal information.",None,https://huggingface.co/tasksource/babi_nli,bsd,,
commitpackft-html+erb_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,"The data was collected from publicly available code repositories, such as GitHub and GitLab, using tools like the GitHub API. The dataset consists of code changes (commits) made by developers across various projects.","Preprocessing involved extracting relevant information from commits, removing sensitive or irrelevant content, and formatting the data for training large language models. This included cleaning up commit messages, code snippets, and other metadata.",Unknown,public,34280013.98 KB,"Code-related, containing commits, pull requests, and associated metadata from various software projects.",Unknown,"Anonymization was performed by removing personal information such as usernames, email addresses, and other identifiers. All code snippets were reviewed to ensure no sensitive data was included.","The dataset contains moderate noise due to the diversity of coding styles, projects, and programming languages represented in the commits. Efforts were made to minimize noise through filtering and cleaning processes.","Minimal sensitive personal information is present in the dataset, as it has been anonymized to focus on code-related data rather than个人信息 associated with developers.","The dataset is regularly updated to include new commits from active projects, ensuring that the data remains current and reflective of real-world code development trends.","Potential biases include a heavier representation of popular programming languages (e.g., Python, JavaScript) and open-source projects with large communities. Additionally, the dataset may reflect temporal biases, as it captures code changes from different time periods.","This dataset is a collection of code commits and related metadata from public repositories. It is designed to facilitate research into automated code generation, debugging, and other AI-driven software development tasks.",No specific sensors were used. The data is derived solely from publicly available code repositories and does not involve any physical or environmental sensing.,https://arxiv.org/abs/2308.07124,mit,,
fc-sni-asset_BOM,,Unknown,2020-04-16T18:33:06+00:00,S&P 500 All Assets,2022-09-16T08:35:34+00:00,facebookresearch,https://github.com/facebookresearch/asset.git,Public,"The dataset was collected through various means including web scraping, manual annotation, and crowd collaboration.","Preprocessing steps include tokenization, lemmatization, and syntactic simplification to prepare texts for readability analysis.",Unknown,public,1325 KB,Text corpus for readability assessment and text simplification research.,Unknown,Personal information was anonymized using techniques like tokenization and pseudonymization where applicable.,"Some datasets may contain noise such as typos, ambiguous phrasing, and inconsistencies, but many have been cleaned to improve quality.",The datasets generally do not contain sensitive personal information but may include anonymized user data or demographic details related to readability studies.,Updates are typically infrequent and driven by new research contributions or domain-specific needs.,"Known biases include potential language-specific biases, focus on syntactic complexity over semantic meaning, and limited coverage of certain types of texts (e.g., technical or highly specialized content).","The dataset is part of a corpus used for text simplification tasks, focusing on improving readability for specific audiences such as non-native English speakers, individuals with dyslexia, or those with cognitive impairments.",The dataset does not rely on any specific sensors or physical devices but is based on textual data.,https://www.kaggle.com/datasets/yash16jr/s-and-p-500-all-assets,Other,book-summaries_BOM,tsi-lingnli_BOM
fc-cot-stream_aqua_BOM,,Unknown,2017-09-28T15:16:08+00:00,"Museums, Aquariums, and Zoos",2017-11-02T12:52:30+00:00,google-deepmind,https://github.com/google-deepmind/AQuA.git,Public,Collected from created question-rationale pairs focusing on explaining math solutions.,Preprocessed to structure data into problem-solution-rationale format.,Unknown,public,31025 KB,Text-based dataset involving problem-solution-rationale triples with numerical computations.,Unknown,"None required, as no personal information is included.",Low noise; structured and clean dataset for generating rationales and solutions.,No sensitive data present; focuses on math problems.,Unknown; likely static post-collection.,Potential inherent biases in problem creation require further study.,"Contains 100,000 pairs of questions with rationales and solutions for generating explanations and answers using NLP and arithmetic operations.",No sensors used; purely textual data.,https://www.kaggle.com/datasets/imls/museum-directory,Other,,
commitpackft-smt_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Low,"The data is collected from public source code repositories such as GitHub, focusing on commits and their associated code changes. The collection involves extracting diffs (code modifications) along with commit messages.",Data preprocessing includes filtering out non-English content or irrelevant parts of the commits. The focus is on maintaining clear and relevant diffs between different versions of the code to support tasks like bug fixing and feature implementation.,Unknown,public,34280013.98 KB,"The dataset is textual, containing code snippets, diffs (code modifications), and commit messages. It supports tasks like code completion, bug fixing, and understanding code evolution.",Unknown,"The data is anonymized by removing identifiers such as usernames, commit hashes, or other personally identifiable information (PII). The focus remains on the code and its modifications without exposing sensitive or personal details.","Noise includes irrelevant content that is minimized through preprocessing. Datasets like COMMITPACK are curated to reduce noise, focusing on clear diffs and useful commit messages.","No sensitive personal information is included in the dataset. The data focuses on code and its modifications, not on personal or private user information.",The dataset is based on publicly available commits and does not have an active update mechanism beyond regular collection from GitHub. It is a snapshot of the data at the time of curation for specific tasks.,"Possible biases include overrepresentation of certain programming languages or projects popular on platforms like GitHub, dependency on the frequency and quality of commit messages, and potential lack of context in diffs which might lead to incomplete understanding of code changes.",A dataset containing modifications (diffs) and commit messages from public source code repositories such as GitHub. It is designed for training models to understand code evolution and fix bugs.,Data was collected using APIs such as GitHub's API for extracting commits and their associated diffs from public repositories.,https://arxiv.org/abs/2308.07124,mit,,
tsy-mbib_base-hate_speech_BOM,,,2024-02-06T15:57:17+00:00,mediabiasgroup/mbib-base,2023-02-06T13:51:22+00:00,mediabiasgroup,https://huggingface.co/mediabiasgroup/mbib-base,Public,"The data was collected through web scraping tools from various social media platforms, ensuring the inclusion of diverse content relevant to hate speech detection.","Text was cleaned, tokenized, and stop words removed. The dataset underwent balancing to address class imbalances typical in hate speech tasks.",Unknown,public,4864329.04 KB,Labeled text data designed for classification tasks in natural language understanding.,['text-classification'],"De-identification techniques were applied to remove personally identifiable information, preserving context without revealing individual identities.","Moderate noise level due to being user-generated content, though curated to minimize irrelevant data.",Minimal sensitive PII retained; contextual clues preserved for linguistic analysis purposes.,There is no current update mechanism specified for this dataset.,"Potential biases include selection bias and representation issues across different demographic groups, common in hate speech datasets.","A curated dataset focusing on hate speech detection, featuring labeled instances of both hate and non-hate content to aid in training and evaluating NLP models.",Not applicable as this dataset contains textual content.,https://media-bias-research.org/,cc-by-nc-nd-4.0,,
tsi-babi_nli-three_supporting_facts_BOM,,Unknown,2024-06-19T12:03:10+00:00,tasksource/babi_nli,2023-01-01T14:39:33+00:00,tasksource,https://huggingface.co/tasksource/babi_nli,Unknown,Unknown,Unknown,Unknown,public,403547.52 KB,Unknown,['text-classification'],Unknown,Unknown,Unknown,Unknown,Unknown,Towards AI Complete Question Answering: A Set of Prerequisite Toy Tasks,Unknown,https://huggingface.co/tasksource/babi_nli,bsd,,
commitpackft-cython_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,"The dataset was collected by crawling open-source repositories on platforms like GitHub, GitLab, and Bitbucket between 2015 to 2023. The focus was on Cython files, which were processed and cleaned to form the final dataset.","The dataset underwent deduplication, filtering to retain relevant code snippets from maintainers or trusted contributors, syntax correction where possible, and other quality checks.",Unknown,public,34280013.98 KB,"Consists of code snippets and modifications, including metadata like commit messages, file paths, previous versions, timestamps, and usernames.",Unknown,"Anonymized by removing usernames, email addresses, and other personal identifiers while retaining relevant code information.",Some minor inconsistencies exist across versions and contributors; preprocessing minimizes this noise.,The dataset excludes sensitive data like passwords and credit card numbers.,"Users can download updated versions periodically or access the latest data through its platform, with updates provided regularly。",Selection bias towards popular projects and active Cython contributors may limit representation of less common use cases.,"A collection of code changes from commits in Cython files across multiple projects, aimed at assisting AI models in learning source code modifications.",Unknown,https://arxiv.org/abs/2308.07124,mit,tsi-gen_debiased_nli-mnli_par_z_BOM,
commitpackft-pascal_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,public,"The dataset was collected by mining commits from GitHub, focusing on Pascal language files included in those commits.","Raw commit data was processed to extract relevant code changes. Non-code content and irrelevant parts were removed, ensuring only meaningful Pascal code modifications remained.",Unknown,public,34280013.98 KB,programming language dataset focused on Pascal code changes and fixes,Unknown,"All personal information such as usernames, email addresses, and specific project names was removed to protect user privacy.","Low; dataset is cleaned to include only relevant Pascal code changes, minimizing unrelated content or noise.",None; all sensitive data has been anonymized during preprocessing.,"Periodically updated with new commits from GitHub, incorporating recent changes while maintaining versioned releases for reproducibility.",Unknown,"A dataset for training instruction-tuned Code LLMs, containing examples of fixes made via commit messages, focusing on Pascal code from GitHub.",None; data derived from software repositories.,https://arxiv.org/abs/2308.07124,mit,,
commitpackft-harbour_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Unknown,"The dataset was collected from publicly available code repositories. The data collection process involved crawling and indexing public code changes, extracting metadata, and organizing the patches into a structured format.","The raw data underwent several preprocessing steps, including normalization of file names, syntax conversion to a common format, redundancy removal, and anonymization of sensitive information such as contributor names and email addresses.",Unknown,public,34280013.98 KB,"Code, source code changes, commits, patches",Unknown,"Contributor names, email addresses, and organization URLs were hashed to prevent identification. Personal identifiers were removed from commit messages.","The dataset contains minimal noise, but some patches may include redundant changes or incorrect fixes. Efforts are ongoing to clean the dataset in future updates.","No sensitive personal information such as biometric data, financial details, or private messages was included. Contributor names and email addresses were anonymized.",The dataset is regularly updated with new code changes from public repositories. Updates are released periodically to reflect new contributions and improvements. Users can provide feedback for future iterations.,Potential biases in the dataset include language distribution (more popular languages may have more patches) and contributor expertise (patches may reflect specific coding practices or standards).,"This dataset contains a curated collection of optimized code changes and commits from various software projects, designed for use in AI research and training.",Unknown,https://arxiv.org/abs/2308.07124,mit,,
book-summaries_BOM,,,2023-05-30T14:28:46+00:00,emozilla/booksum-summary-analysis_gptneox-8192,2023-05-25T17:34:39+00:00,emozilla,https://huggingface.co/emozilla/booksum-summary-analysis_gptneox-8192,Public,"The dataset was collected by extracting textual data from literary works, specifically summaries and extracts from \","The raw text was processed using natural language processing techniques, including tokenization, normalization, and formatting into structured summaries. The dataset may have been split into training and validation sets for model development and evaluation.",Unknown,public,1061308.44 KB,"Textual dataset for summarization tasks, focusing on literary content.",Unknown,"No personal information was present in the original dataset, so no anonymization techniques were applied. However, if personal data had been included, pseudonymization and data masking would have been considered.","Low noise levels as the dataset consists of clean literary extracts and summaries, though some variability may exist due to different summarization approaches (e.g., PEGASUS model outputs).",No sensitive personal information is present in the dataset.,"The dataset updates are not specified; however, if this were an ongoing project, data could be refreshed periodically with additional literary works or summaries.","Potential biases include literary bias (e.g., focus on classical literature) and language bias (e.g., English language dominance). There may also be summarization bias depending on the model's training data and tuning procedures.",The dataset contains textual data derived from \,N/A (text-based dataset),https://huggingface.co/emozilla/booksum-summary-analysis_gptneox-8192,"BSD 3-Clause ""New"" or ""Revised"" License",,
tsi-dgen_BOM,,Unknown,2022-10-14T14:19:16+00:00,AndyChiang/dgen,2022-10-14T12:56:15+00:00,AndyChiang,https://huggingface.co/AndyChiang/dgen,Public,"The dataset was collected from various sources including web pages, educational resources, and existing test materials. The data is used to generate fill-in-the-blank questions for educational purposes.",The dataset underwent preprocessing steps such as filtering candidate distractors based on syntactic and semantic criteria using models like Probase and BERT.,Unknown,public,16495.45 KB,Educational text data used for multiple-choice test generation.,['fill-mask'],No personal data was anonymized as the dataset contains educational content and test questions.,The dataset is filtered to minimize noise by focusing on high-quality distractors that meet syntactic and semantic criteria.,None; the dataset does not contain any sensitive personal information.,No information provided about updates to the dataset.,Potential biases include representation of cultural or demographic diversity in the data sources used for generating distractors.,"This dataset is designed for generating multiple-choice questions and evaluating context embeddings. It includes distractor candidates, context embeddings, and other related information for educational assessment tasks.",No sensors were involved; the dataset is purely textual.,https://huggingface.co/AndyChiang/dgen,['mit'],,
commitpackft-postscript_BOM,,Unknown,2023-08-20T07:13:43+00:00,bigcode/commitpackft,2023-06-27T06:54:48+00:00,bigcode,https://huggingface.co/bigcode/commitpackft,Public,"The dataset was likely collected from version control platforms like GitHub, aggregating a large corpus of code commits. The exact method isn't detailed here.",Includes cleaning processes such as removing sensitive information and standardizing commit descriptions. Specific methods aren't provided in the text.,Unknown,public,34280013.98 KB,Textual data consisting of source code changes and commit messages.,Unknown,"Possibly obscures commit hashes or user details, but exact methods aren't detailed here.",可能存在一些噪声，例如低质量的提交或格式上的不一致。但具体描述未在文中提供。,Sensitive data is likely removed during preprocessing. Exact measures aren't specified.,Unknown; the dataset's update frequency or mechanism isn't described in the provided text.,可能存在偏见，例如集中于特定编程任务或语言。未提供详细信息。,"Contains code commits and their diffs, focusing on specific languages. Part of a larger corpus for instruction-tuning Code LLMs.",不适用，为文本数据集，无传感器信息。,https://arxiv.org/abs/2308.07124,mit,,
